import streamlit as st
import os
import sys
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import glob
from typing import List, Dict, Tuple
import requests
import json
import PyPDF2
import fitz  # PyMuPDF for better PDF handling
from PIL import Image
import io

# Page configuration - MUST be first Streamlit command
st.set_page_config(
    page_title="Rory's AI Assistant - Enhanced Reasoning",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

class EnhancedDocumentProcessor:
    """Enhanced document processor with document type separation"""
    
    def __init__(self):
        self.ocr_available = False
        # Try to import OCR dependencies
        try:
            import pytesseract
            pytesseract.get_tesseract_version()
            self.ocr_available = True
            st.success("âœ… OCR capabilities available - PDF images can be processed")
        except Exception as e:
            st.warning(f"âš ï¸ OCR not available: {str(e)}. PDF text extraction will use standard methods only.")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using available methods"""
        text = ""
        
        try:
            # Method 1: Try PyMuPDF first (best for text-based PDFs)
            doc = fitz.open(pdf_path)
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                if page_text.strip():
                    text += page_text + "\n"
                elif self.ocr_available:
                    # If OCR is available and no text found, try OCR on this page
                    try:
                        import pytesseract
                        pix = page.get_pixmap()
                        img_data = pix.tobytes("png")
                        img = Image.open(io.BytesIO(img_data))
                        ocr_text = pytesseract.image_to_string(img, lang='chi_sim+eng')
                        if ocr_text.strip():
                            text += f"[OCR Page {page_num + 1}]\n{ocr_text}\n"
                    except Exception as ocr_error:
                        st.warning(f"OCR failed for page {page_num + 1}: {str(ocr_error)}")
            doc.close()
            
            # If we got good text, return it
            if len(text.strip()) > 100:
                return text
                
        except Exception as e:
            st.warning(f"PyMuPDF extraction failed: {str(e)}")
        
        try:
            # Method 2: Fallback to PyPDF2
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text.strip():
                        text += page_text + "\n"
            
            if len(text.strip()) > 100:
                return text
                
        except Exception as e:
            st.warning(f"PyPDF2 extraction failed: {str(e)}")
        
        if self.ocr_available:
            try:
                # Method 3: Full OCR as last resort
                import pytesseract
                st.info(f"Attempting OCR extraction for {os.path.basename(pdf_path)}...")
                doc = fitz.open(pdf_path)
                for page_num in range(min(10, len(doc))):  # Limit to first 10 pages for performance
                    page = doc.load_page(page_num)
                    pix = page.get_pixmap()
                    img_data = pix.tobytes("png")
                    img = Image.open(io.BytesIO(img_data))
                    
                    # Try OCR with Chinese and English
                    ocr_text = pytesseract.image_to_string(img, lang='chi_sim+eng')
                    if ocr_text.strip():
                        text += f"[OCR Page {page_num + 1}]\n{ocr_text}\n"
                doc.close()
                
                if len(text.strip()) > 50:
                    return text
                    
            except Exception as e:
                st.error(f"OCR extraction failed: {str(e)}")
        
        return f"Failed to extract readable text from {os.path.basename(pdf_path)}. The file may contain only images or be corrupted. {'OCR is not available to process image-based content.' if not self.ocr_available else ''}"
    
    def process_documents_with_separation(self, directory: str) -> Dict[str, List[Tuple[str, Dict]]]:
        """Process documents with type separation"""
        documents = {
            "professional": [],  # CV and work-related documents
            "mindset": [],       # Flomo document for tone/style learning
            "general": []        # Other documents
        }
        
        # Process text files with categorization
        text_extensions = ['*.txt', '*.md']
        for ext in text_extensions:
            files = glob.glob(os.path.join(directory, ext))
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    if text.strip() and len(text) > 100:
                        filename = os.path.basename(file_path).lower()
                        
                        # Categorize documents
                        if 'cv' in filename or 'resume' in filename:
                            doc_type = "professional"
                        elif 'flomo' in filename:
                            doc_type = "mindset"
                        else:
                            doc_type = "general"
                        
                        documents[doc_type].append((text, {
                            "source": os.path.basename(file_path), 
                            "type": "text",
                            "path": file_path,
                            "category": doc_type
                        }))
                        
                except Exception as e:
                    st.warning(f"Failed to read {file_path}: {str(e)}")
        
        # Process PDF files
        pdf_files = glob.glob(os.path.join(directory, '*.pdf'))
        for pdf_path in pdf_files:
            try:
                st.info(f"Processing PDF: {os.path.basename(pdf_path)}")
                text = self.extract_text_from_pdf(pdf_path)
                if text.strip() and len(text) > 50:
                    filename = os.path.basename(pdf_path).lower()
                    
                    # Categorize PDFs
                    if 'cv' in filename or 'resume' in filename:
                        doc_type = "professional"
                    elif 'flomo' in filename:
                        doc_type = "mindset"
                    else:
                        doc_type = "general"
                    
                    documents[doc_type].append((text, {
                        "source": os.path.basename(pdf_path), 
                        "type": "pdf",
                        "path": pdf_path,
                        "category": doc_type
                    }))
                    st.success(f"Successfully processed {os.path.basename(pdf_path)}")
                else:
                    st.warning(f"No readable text found in {os.path.basename(pdf_path)}")
            except Exception as e:
                st.error(f"Failed to process PDF {pdf_path}: {str(e)}")
        
        return documents

class EnhancedVectorStore:
    """Enhanced vector store with document type awareness"""
    
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.documents = {
            "professional": [],
            "mindset": [],
            "general": []
        }
        self.embeddings = {
            "professional": [],
            "mindset": [],
            "general": []
        }
        self.metadata = {
            "professional": [],
            "mindset": [],
            "general": []
        }
    
    def add_documents_by_type(self, documents_dict: Dict[str, List[Tuple[str, Dict]]]):
        """Add documents categorized by type"""
        for doc_type, docs in documents_dict.items():
            for text, metadata in docs:
                if text.strip():
                    # Optimized chunking
                    chunks = self._split_text(text, chunk_size=300, overlap=30)
                    for i, chunk in enumerate(chunks):
                        if chunk.strip() and len(chunk) > 50:
                            self.documents[doc_type].append(chunk)
                            chunk_metadata = metadata.copy()
                            chunk_metadata['chunk_index'] = i
                            self.metadata[doc_type].append(chunk_metadata)
        
        # Generate embeddings for each document type
        for doc_type in self.documents:
            if self.documents[doc_type]:
                self.embeddings[doc_type] = self.model.encode(
                    self.documents[doc_type], 
                    batch_size=32, 
                    show_progress_bar=False
                )
    
    def _split_text(self, text: str, chunk_size: int = 300, overlap: int = 30) -> List[str]:
        """Optimized text splitting"""
        sentences = text.replace('\n', ' ').split('. ')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            sentence_length = len(sentence.split())
            
            if current_length + sentence_length > chunk_size and current_chunk:
                chunks.append('. '.join(current_chunk) + '.')
                # Keep some overlap
                overlap_sentences = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                current_chunk = overlap_sentences + [sentence]
                current_length = sum(len(s.split()) for s in current_chunk)
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        if current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
        
        return chunks if chunks else [text]
    
    def similarity_search_by_type(self, query: str, doc_type: str = "professional", k: int = 3) -> List[Tuple[str, Dict]]:
        """Search within specific document type"""
        if not self.documents[doc_type]:
            return []
        
        query_embedding = self.model.encode([query], show_progress_bar=False)
        similarities = cosine_similarity(query_embedding, self.embeddings[doc_type])[0]
        
        # Get top k results with higher threshold for better quality
        top_indices = np.argsort(similarities)[::-1][:k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.15:
                results.append((self.documents[doc_type][idx], self.metadata[doc_type][idx]))
        
        return results

class AdvancedReasoningChain:
    """Advanced reasoning system with enhanced analytical capabilities"""
    
    def __init__(self, vector_store: EnhancedVectorStore):
        self.vector_store = vector_store
        self.api_key = "sk-015ea57c8b254c4181d30b2de4259d8b"
        self.api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        self.reasoning_cache = {}
    
    def _analyze_query_complexity(self, query: str) -> Dict:
        """Analyze query complexity and reasoning requirements"""
        query_lower = query.lower()
        
        analysis = {
            "reasoning_type": "factual",
            "complexity": "simple",
            "requires_inference": False,
            "requires_comparison": False,
            "requires_analysis": False,
            "context_sources": ["professional"],
            "reasoning_steps": []
        }
        
        # Detect reasoning requirements
        if any(word in query_lower for word in ['why', 'how', 'explain', 'analyze', 'reason']):
            analysis["reasoning_type"] = "analytical"
            analysis["complexity"] = "complex"
            analysis["requires_analysis"] = True
            analysis["reasoning_steps"].append("causal_analysis")
        
        if any(word in query_lower for word in ['compare', 'versus', 'difference', 'better', 'worse']):
            analysis["requires_comparison"] = True
            analysis["reasoning_steps"].append("comparative_analysis")
        
        if any(word in query_lower for word in ['predict', 'future', 'trend', 'likely', 'expect']):
            analysis["requires_inference"] = True
            analysis["reasoning_steps"].append("predictive_reasoning")
        
        if any(word in query_lower for word in ['strategy', 'approach', 'methodology', 'framework']):
            analysis["reasoning_type"] = "strategic"
            analysis["reasoning_steps"].append("strategic_thinking")
        
        # Determine context sources needed
        if any(word in query_lower for word in ['experience', 'work', 'job', 'career', 'professional', 'skill']):
            analysis["context_sources"] = ["professional"]
        elif any(word in query_lower for word in ['personality', 'character', 'mindset', 'thinking']):
            analysis["context_sources"] = ["professional", "mindset"]  # Use both but filter mindset content
        else:
            analysis["context_sources"] = ["professional", "general"]
        
        return analysis
    
    def _extract_mindset_patterns(self, mindset_content: str) -> Dict:
        """Extract communication patterns and tone from mindset content without revealing personal details"""
        patterns = {
            "communication_style": "thoughtful and reflective",
            "analytical_approach": "systematic and thorough",
            "tone_characteristics": ["professional", "curious", "growth-oriented"],
            "thinking_patterns": ["analytical", "philosophical", "practical"]
        }
        
        # Analyze patterns without exposing personal content
        if "åˆ†æ" in mindset_content or "æ€è€ƒ" in mindset_content:
            patterns["analytical_approach"] = "deep analytical thinking"
        
        if "å­¸ç¿’" in mindset_content or "æˆé•·" in mindset_content:
            patterns["tone_characteristics"].append("learning-focused")
        
        if "ç¶“é©—" in mindset_content or "ç¸½çµ" in mindset_content:
            patterns["thinking_patterns"].append("experience-based reasoning")
        
        return patterns
    
    def _perform_advanced_reasoning(self, query: str, context: str, analysis: Dict, mindset_patterns: Dict) -> str:
        """Perform advanced reasoning based on query analysis"""
        reasoning_steps = []
        
        # Step 1: Information extraction
        reasoning_steps.append("**ä¿¡æ¯æå–èˆ‡åˆ†æ**")
        reasoning_steps.append(f"åŸºæ–¼Roryçš„å°ˆæ¥­å±¥æ­·ï¼Œæˆ‘è­˜åˆ¥å‡ºä»¥ä¸‹é—œéµä¿¡æ¯ï¼š")
        
        # Step 2: Pattern recognition
        if analysis["requires_analysis"]:
            reasoning_steps.append("\n**æ¨¡å¼è­˜åˆ¥èˆ‡æ·±åº¦åˆ†æ**")
            reasoning_steps.append("é€šéåˆ†æRoryçš„è·æ¥­ç™¼å±•è»Œè·¡ï¼Œæˆ‘ç™¼ç¾ä»¥ä¸‹æ¨¡å¼ï¼š")
            
            # Analyze career progression
            if "data science" in context.lower():
                reasoning_steps.append("â€¢ **æŠ€è¡“æ¼”é€²è·¯å¾‘**: å¾æ•¸æ“šåˆ†æå¸«åˆ°åŠ©ç†ç¶“ç†å†åˆ°AVPï¼Œå±•ç¾äº†æŠ€è¡“æ·±åº¦èˆ‡ç®¡ç†å»£åº¦çš„é›™é‡ç™¼å±•")
                reasoning_steps.append("â€¢ **è¡Œæ¥­æ´å¯Ÿèƒ½åŠ›**: è·¨è¶Šä¿éšªã€é†«ç™‚ã€éŠ€è¡Œä¸‰å€‹è¡Œæ¥­ï¼Œç©ç´¯äº†è±å¯Œçš„æ¥­å‹™ç†è§£")
                reasoning_steps.append("â€¢ **å‰µæ–°é©…å‹•æ€ç¶­**: å¤šæ¬¡ç²å¾—å‰µæ–°çé …ï¼Œèªªæ˜å…·å‚™å‰ç»æ€§æ€ç¶­å’Œå¯¦è¸èƒ½åŠ›")
        
        # Step 3: Comparative analysis
        if analysis["requires_comparison"]:
            reasoning_steps.append("\n**æ¯”è¼ƒåˆ†æèˆ‡å·®ç•°åŒ–å„ªå‹¢**")
            reasoning_steps.append("ç›¸æ¯”åŒè¡Œæ¥­å°ˆæ¥­äººå£«ï¼ŒRoryçš„ç¨ç‰¹å„ªå‹¢åœ¨æ–¼ï¼š")
            reasoning_steps.append("â€¢ **è·¨è¡Œæ¥­ç¶“é©—**: é†«ç™‚ä¿éšªâ†’ä¿éšªâ†’éŠ€è¡Œçš„å®Œæ•´é‡‘èæœå‹™éˆæ¢ç¶“é©—")
            reasoning_steps.append("â€¢ **æŠ€è¡“èˆ‡æ¥­å‹™å¹³è¡¡**: æ—¢æœ‰æ·±åº¦æŠ€è¡“èƒ½åŠ›ï¼Œåˆå…·å‚™æ¥­å‹™æ´å¯Ÿå’Œç®¡ç†ç¶“é©—")
            reasoning_steps.append("â€¢ **æŒçºŒå­¸ç¿’èƒ½åŠ›**: é›™ç¢©å£«å­¸ä½èƒŒæ™¯é«”ç¾äº†æŒçºŒå­¸ç¿’å’ŒçŸ¥è­˜æ›´æ–°çš„èƒ½åŠ›")
        
        # Step 4: Strategic reasoning
        if "strategic" in analysis["reasoning_type"]:
            reasoning_steps.append("\n**æˆ°ç•¥æ€ç¶­èˆ‡æ–¹æ³•è«–**")
            reasoning_steps.append("åŸºæ–¼Roryçš„ç¶“é©—ï¼Œæˆ‘æ¨æ–·å…¶è§£æ±ºå•é¡Œçš„æ–¹æ³•è«–åŒ…æ‹¬ï¼š")
            reasoning_steps.append("â€¢ **æ•¸æ“šé©…å‹•æ±ºç­–**: åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’å’Œçµ±è¨ˆåˆ†ææ”¯æ’æ¥­å‹™æ±ºç­–")
            reasoning_steps.append("â€¢ **æ•æ·è¿­ä»£å„ªåŒ–**: é€šéA/Bæ¸¬è©¦å’Œæ¨¡å‹ç›£æ§æŒçºŒå„ªåŒ–æ•ˆæœ")
            reasoning_steps.append("â€¢ **è·¨éƒ¨é–€å”ä½œ**: å…·å‚™èˆ‡æ¥­å‹™ã€ITã€åˆè¦ç­‰å¤šéƒ¨é–€å”ä½œçš„ç¶“é©—")
        
        # Step 5: Predictive reasoning
        if analysis["requires_inference"]:
            reasoning_steps.append("\n**é æ¸¬æ€§æ¨ç†èˆ‡è¶¨å‹¢åˆ¤æ–·**")
            reasoning_steps.append("åŸºæ–¼Roryçš„æŠ€è¡“ç™¼å±•è»Œè·¡ï¼Œå¯ä»¥æ¨æ–·ï¼š")
            reasoning_steps.append("â€¢ **AI/MLå°ˆç²¾æ–¹å‘**: å¾å‚³çµ±çµ±è¨ˆåˆ†æå‘æ·±åº¦å­¸ç¿’å’Œç”Ÿæˆå¼AIç™¼å±•")
            reasoning_steps.append("â€¢ **æ¥­å‹™åƒ¹å€¼å‰µé€ **: æ³¨é‡æŠ€è¡“èˆ‡æ¥­å‹™çµåˆï¼Œå‰µé€ å¯è¡¡é‡çš„å•†æ¥­åƒ¹å€¼")
            reasoning_steps.append("â€¢ **é ˜å°åŠ›ç™¼å±•**: å¾æŠ€è¡“å°ˆå®¶å‘æŠ€è¡“ç®¡ç†è€…çš„æˆåŠŸè½‰å‹")
        
        # Step 6: Synthesis and conclusion
        reasoning_steps.append("\n**ç¶œåˆæ¨ç†èˆ‡çµè«–**")
        reasoning_steps.append("ç¶œåˆä»¥ä¸Šåˆ†æï¼Œæˆ‘çš„æ¨ç†çµè«–æ˜¯ï¼š")
        
        return "\n".join(reasoning_steps)
    
    def _generate_enhanced_response(self, query: str, professional_context: str, analysis: Dict, mindset_patterns: Dict) -> str:
        """Generate enhanced response with advanced reasoning"""
        
        # Perform advanced reasoning
        reasoning_analysis = self._perform_advanced_reasoning(query, professional_context, analysis, mindset_patterns)
        
        # Create sophisticated prompt
        prompt = f"""ä½ æ˜¯Roryçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å‚™é«˜ç´šæ¨ç†èƒ½åŠ›ã€‚ä½ éœ€è¦åŸºæ–¼å°ˆæ¥­ä¿¡æ¯é€²è¡Œæ·±åº¦åˆ†æå’Œæ¨ç†ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ä¿¡æ¯æå–ã€‚

**æŸ¥è©¢åˆ†æçµæœ**:
- æ¨ç†é¡å‹: {analysis['reasoning_type']}
- è¤‡é›œåº¦: {analysis['complexity']}
- éœ€è¦æ¨ç†æ­¥é©Ÿ: {', '.join(analysis['reasoning_steps'])}

**å°ˆæ¥­èƒŒæ™¯ä¿¡æ¯**:
{professional_context}

**ç”¨æˆ¶å•é¡Œ**: {query}

**æ¨ç†åˆ†æéç¨‹**:
{reasoning_analysis}

**å›ç­”è¦æ±‚**:
1. é€²è¡Œæ·±åº¦æ¨ç†åˆ†æï¼Œä¸åƒ…æå–ä¿¡æ¯ï¼Œæ›´è¦åˆ†æèƒŒå¾Œçš„é‚è¼¯å’Œæ¨¡å¼
2. å±•ç¤ºæ¨ç†éç¨‹ï¼Œèªªæ˜å¦‚ä½•å¾å·²çŸ¥ä¿¡æ¯å¾—å‡ºçµè«–
3. æä¾›æ´å¯Ÿæ€§è§€é»ï¼Œè€Œéç°¡å–®çš„äº‹å¯¦é™³è¿°
4. æ¡ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„èªè¨€é¢¨æ ¼
5. å¦‚æœæ¶‰åŠé æ¸¬æˆ–å»ºè­°ï¼Œè¦åŸºæ–¼é‚è¼¯æ¨ç†çµ¦å‡ºä¾æ“š
6. ä¿æŒå®¢è§€å’Œå°ˆæ¥­ï¼Œé¿å…éåº¦æ¨æ¸¬

è«‹åŸºæ–¼ä»¥ä¸Šåˆ†ææä¾›ä¸€å€‹æ·±åº¦ã€æœ‰æ´å¯ŸåŠ›çš„å›ç­”ï¼š"""

        return prompt
    
    def __call__(self, inputs: Dict) -> Dict:
        """Enhanced reasoning processing"""
        query = inputs.get("query", "").strip()
        
        # Analyze query complexity and reasoning requirements
        analysis = self._analyze_query_complexity(query)
        
        # Get professional context (primary source for work-related queries)
        professional_docs = self.vector_store.similarity_search_by_type(
            query, "professional", k=5 if analysis["complexity"] == "complex" else 3
        )
        professional_context = "\n\n".join([doc[0] for doc in professional_docs])
        
        # Get mindset patterns for tone/style (without exposing personal details)
        mindset_patterns = {}
        if "mindset" in analysis["context_sources"]:
            mindset_docs = self.vector_store.similarity_search_by_type(query, "mindset", k=2)
            if mindset_docs:
                mindset_content = "\n".join([doc[0] for doc in mindset_docs])
                mindset_patterns = self._extract_mindset_patterns(mindset_content)
        
        # Check cache for complex reasoning
        cache_key = hash(query.lower().strip() + str(analysis) + professional_context[:300])
        if cache_key in self.reasoning_cache:
            return {"result": self.reasoning_cache[cache_key]}
        
        # Generate enhanced prompt with reasoning
        prompt = self._generate_enhanced_response(query, professional_context, analysis, mindset_patterns)
        
        # Try API call
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": "qwen-plus",
                "input": {
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "temperature": 0.7,
                    "max_tokens": 1500,
                    "top_p": 0.9
                }
            }
            
            response = requests.post(self.api_url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'output' in result and 'choices' in result['output']:
                    answer = result['output']['choices'][0]['message']['content']
                    self.reasoning_cache[cache_key] = answer
                    return {"result": answer}
                    
        except Exception as e:
            print(f"API Error: {str(e)}")
        
        # Enhanced fallback with reasoning
        if professional_context.strip():
            # Perform local reasoning analysis
            reasoning_analysis = self._perform_advanced_reasoning(query, professional_context, analysis, mindset_patterns)
            
            fallback_response = f"""**åŸºæ–¼å¯ç”¨è³‡æ–™çš„æ·±åº¦åˆ†æ**

{reasoning_analysis}

**å°ˆæ¥­èƒŒæ™¯æ‘˜è¦**:
{professional_context[:800]}...

**åˆ†æçµè«–**:
åŸºæ–¼ä»¥ä¸Šæ¨ç†åˆ†æï¼ŒRoryåœ¨æ•¸æ“šç§‘å­¸é ˜åŸŸå±•ç¾å‡ºäº†å¾æŠ€è¡“å°ˆå®¶åˆ°æ¥­å‹™é ˜å°è€…çš„å®Œæ•´ç™¼å±•è»Œè·¡ã€‚å…¶ç¨ç‰¹åƒ¹å€¼åœ¨æ–¼è·¨è¡Œæ¥­ç¶“é©—ã€æŠ€è¡“èˆ‡æ¥­å‹™çš„å¹³è¡¡èƒ½åŠ›ï¼Œä»¥åŠæŒçºŒå‰µæ–°çš„æ€ç¶­æ¨¡å¼ã€‚

*æ­¤åˆ†æåŸºæ–¼å°ˆæ¥­å±¥æ­·ä¿¡æ¯ï¼Œé€šéé‚è¼¯æ¨ç†å’Œæ¨¡å¼è­˜åˆ¥å¾—å‡ºã€‚å¦‚éœ€æ›´å…·é«”çš„ä¿¡æ¯ï¼Œæ­¡è¿æå‡ºæ›´è©³ç´°çš„å•é¡Œã€‚*"""
        else:
            fallback_response = """**æ­¡è¿ä½¿ç”¨Roryçš„å¢å¼·æ¨ç†åŠ©æ‰‹**

æˆ‘å…·å‚™é«˜ç´šæ¨ç†åˆ†æèƒ½åŠ›ï¼Œå¯ä»¥ï¼š

ğŸ§  **æ·±åº¦æ¨ç†**: ä¸åƒ…æå–ä¿¡æ¯ï¼Œæ›´åˆ†æèƒŒå¾Œçš„é‚è¼¯å’Œæ¨¡å¼
ğŸ“Š **æ¯”è¼ƒåˆ†æ**: è­˜åˆ¥å„ªå‹¢ã€å·®ç•°å’Œç™¼å±•è¶¨å‹¢  
ğŸ”® **é æ¸¬æ¨ç†**: åŸºæ–¼ç¾æœ‰ä¿¡æ¯é€²è¡Œåˆç†æ¨æ¸¬
ğŸ’¡ **æˆ°ç•¥æ€ç¶­**: æä¾›æ–¹æ³•è«–å’Œè§£æ±ºæ–¹æ¡ˆå»ºè­°

**è©¦è©¦é€™äº›æ·±åº¦å•é¡Œ**:
â€¢ "åˆ†æRoryçš„è·æ¥­ç™¼å±•æ¨¡å¼å’ŒæˆåŠŸå› ç´ "
â€¢ "æ¯”è¼ƒRoryåœ¨ä¸åŒè¡Œæ¥­çš„ç¶“é©—ï¼Œæœ‰ä»€éº¼ç¨ç‰¹å„ªå‹¢ï¼Ÿ"
â€¢ "åŸºæ–¼Roryçš„èƒŒæ™¯ï¼Œé æ¸¬ä»–åœ¨AIé ˜åŸŸçš„ç™¼å±•æ–¹å‘"
â€¢ "Roryè§£æ±ºè¤‡é›œæ•¸æ“šç§‘å­¸å•é¡Œçš„æ–¹æ³•è«–æ˜¯ä»€éº¼ï¼Ÿ"

*æˆ‘æœƒé€²è¡Œæ·±åº¦åˆ†æå’Œæ¨ç†ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ä¿¡æ¯æª¢ç´¢ã€‚*"""
        
        self.reasoning_cache[cache_key] = fallback_response
        return {"result": fallback_response}

@st.cache_resource
def initialize_enhanced_reasoning_system():
    """Initialize enhanced reasoning system with document separation"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Initialize document processor
        doc_processor = EnhancedDocumentProcessor()
        
        # Process documents with type separation
        documents_dict = doc_processor.process_documents_with_separation(current_dir)
        
        # Enhanced fallback content if no documents found
        if not any(documents_dict.values()):
            fallback_content = """
            Rory Chen - Data Science & Analytics Expert
            
            Professional Experience:
            AVP, Data Science at China CITIC Bank International (Nov 2022 - Current)
            - Led data science initiatives in retail banking with AI-driven decision-making
            - Developed 20+ ML models achieving 1.5x business uplift vs control group
            - Created AutoML pipeline reducing coding effort by 80%
            - Implemented Next-Best-Action (NBA) model framework
            
            Assistant Data Science Manager at AXA Hong Kong and Macau (Sep 2019 - Nov 2022)
            - Advanced data capabilities for insurance analytics and predictive modeling
            - Designed AI+BI framework and ONE AXA Dashboard (100+ monthly users)
            - Led cloud migration from on-premise to Azure infrastructure
            - Implemented AI model governance framework
            
            Data Science Analyst at Cigna International Office (Aug 2018 - Sep 2019)
            - Supported US Medicare analytics with predictive modeling focus
            - Developed healthcare cost forecasting and risk assessment models
            
            Education:
            Master of Arts in Quantitative Analysis for Business, City University of Hong Kong (2016-2017)
            Master of Arts in Public Policy and Management, City University of Hong Kong (2014-2015)
            Bachelor of Social Sciences (Honours), Education University of Hong Kong (2011-2014)
            
            Technical Skills:
            Programming: Python, R, SQL (Advanced proficiency)
            Machine Learning: Deep Learning, NLP, Computer Vision, MLOps
            Analytics: Predictive Modeling, Customer Analytics, Time Series Analysis
            Cloud Platforms: Azure, AWS, Databricks, Cloudera CDSW
            Visualization: Tableau, Power BI, Dashboard Development
            Tools: Spark, Hadoop, Docker, Kubernetes, AutoML
            
            Contact: chengy823@gmail.com
            Location: Hong Kong SAR
            Languages: Cantonese (Native), Mandarin (Native), English (Proficient)
            """
            documents_dict["professional"] = [(fallback_content, {"source": "profile", "type": "text", "category": "professional"})]
        
        # Create enhanced vector store
        vector_store = EnhancedVectorStore()
        vector_store.add_documents_by_type(documents_dict)
        
        # Create advanced reasoning chain
        reasoning_chain = AdvancedReasoningChain(vector_store)
        
        # Calculate total documents and chunks
        total_docs = sum(len(docs) for docs in documents_dict.values())
        total_chunks = sum(len(chunks) for chunks in vector_store.documents.values())
        
        return reasoning_chain, total_docs, total_chunks, documents_dict
    
    except Exception as e:
        st.error(f"Failed to initialize enhanced reasoning system: {str(e)}")
        return None, 0, 0, {}

# Custom CSS for professional styling
st.markdown("""
<style>
    .main .block-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        font-size: 16px;
        line-height: 1.6;
    }
    
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    
    .main-header h1 {
        font-size: 2.5rem !important;
        font-weight: 700 !important;
        margin-bottom: 0.5rem !important;
    }
    
    .main-header h3 {
        font-size: 1.5rem !important;
        font-weight: 400 !important;
        margin-bottom: 1rem !important;
    }
    
    .main-header p {
        font-size: 1.1rem !important;
        opacity: 0.9;
    }
    
    .status-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        font-size: 1rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    
    .reasoning-badge {
        background: linear-gradient(45deg, #ff6b6b, #feca57);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: bold;
        display: inline-block;
        margin: 0.5rem 0;
    }
    
    .chat-container {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    
    .stButton > button {
        width: 100%;
        border-radius: 8px;
        border: 1px solid #ddd;
        background: white;
        color: #333;
        padding: 0.75rem 1rem;
        margin: 0.3rem 0;
        transition: all 0.3s;
        font-size: 0.95rem;
        font-weight: 500;
        text-align: left;
    }
    
    .stButton > button:hover {
        background: #f0f8ff;
        border-color: #667eea;
        transform: translateY(-1px);
        box-shadow: 0 2px 8px rgba(102, 126, 234, 0.2);
    }
    
    .stChatMessage {
        font-size: 1rem !important;
        line-height: 1.6 !important;
    }
    
    .stChatMessage [data-testid="chatAvatarIcon-user"] {
        background-color: #667eea !important;
    }
    
    .stChatMessage [data-testid="chatAvatarIcon-assistant"] {
        background-color: #764ba2 !important;
    }
    
    .stTextInput > div > div > input {
        font-size: 1rem !important;
        padding: 0.75rem !important;
    }
</style>
""", unsafe_allow_html=True)

# Main header
st.markdown("""
<div class="main-header">
    <h1>ğŸ§  Rory's Enhanced Reasoning Assistant</h1>
    <h3>Advanced AI with Deep Analytical Capabilities</h3>
    <p>Ask complex questions and get sophisticated reasoning-based answers!</p>
</div>
""", unsafe_allow_html=True)

# Initialize enhanced reasoning system
with st.spinner("ğŸš€ Initializing Enhanced Reasoning System..."):
    reasoning_chain, total_docs, total_chunks, documents_dict = initialize_enhanced_reasoning_system()

if reasoning_chain is None:
    st.error("âŒ Enhanced reasoning system is not available. Please check the system configuration.")
    st.stop()

# Display system status with document breakdown
professional_count = len(documents_dict.get("professional", []))
mindset_count = len(documents_dict.get("mindset", []))
general_count = len(documents_dict.get("general", []))

st.markdown(f"""
<div class="status-info">
    <strong>ğŸ§  Enhanced Reasoning System Status:</strong><br>
    ğŸ“„ <strong>Documents Processed:</strong> {total_docs} total ({professional_count} professional, {mindset_count} mindset, {general_count} general)<br>
    ğŸ§© <strong>Knowledge Chunks:</strong> {total_chunks} available for reasoning<br>
    <span class="reasoning-badge">ğŸ” Advanced Reasoning Enabled</span>
    <span class="reasoning-badge">ğŸ“Š Document Type Separation</span>
    <span class="reasoning-badge">ğŸ¯ Professional Focus</span>
</div>
""", unsafe_allow_html=True)

# Create two columns for layout
col1, col2 = st.columns([1, 1])

with col1:
    # Enhanced CV Section
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                padding: 2rem; border-radius: 15px; color: white; margin-bottom: 2rem;
                box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <div style="text-align: center; margin-bottom: 1.5rem;">
            <h1 style="color: white; margin-bottom: 0.5rem;">ğŸ‘¨â€ğŸ’¼ Rory Chen</h1>
            <h3 style="color: #e8f4fd; margin-bottom: 0.5rem;">Data Science & Analytics Expert</h3>
            <p style="color: #b8d4f0; font-size: 1.1rem;">ğŸ¢ China CITIC Bank International Limited</p>
            <p style="color: #b8d4f0;">ğŸ“ Hong Kong SAR | ğŸ§  Enhanced Reasoning AI</p>
        </div>
    </div>
    
    <div style="background: white; padding: 2rem; border-radius: 15px; 
                box-shadow: 0 4px 20px rgba(0,0,0,0.08); margin-bottom: 2rem;">
        <h2 style="color: #2c3e50; border-bottom: 3px solid #667eea; padding-bottom: 0.5rem;">ğŸ§  Reasoning Capabilities</h2>
        <div style="margin: 1rem 0;">
            <div style="margin-bottom: 1rem;">
                <h4 style="color: #667eea; margin-bottom: 0.3rem;">ğŸ” Deep Analysis</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Analyzes patterns, identifies trends, and provides insights beyond surface-level information</p>
            </div>
            <div style="margin-bottom: 1rem;">
                <h4 style="color: #764ba2; margin-bottom: 0.3rem;">ğŸ“Š Comparative Reasoning</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Compares experiences across industries, identifies unique advantages and differentiators</p>
            </div>
            <div style="margin-bottom: 1rem;">
                <h4 style="color: #ff6b6b; margin-bottom: 0.3rem;">ğŸ”® Predictive Insights</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Makes informed predictions about career trends and technology directions</p>
            </div>
            <div>
                <h4 style="color: #feca57; margin-bottom: 0.3rem;">ğŸ’¡ Strategic Thinking</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Provides methodological approaches and strategic frameworks for problem-solving</p>
            </div>
        </div>
    </div>
    
    <div style="background: white; padding: 2rem; border-radius: 15px; 
                box-shadow: 0 4px 20px rgba(0,0,0,0.08);">
        <h2 style="color: #2c3e50; border-bottom: 3px solid #f39c12; padding-bottom: 0.5rem;">ğŸ“‹ Document Sources</h2>
        <div style="margin: 1rem 0;">
            <div style="margin-bottom: 1rem;">
                <h4 style="color: #27ae60; margin-bottom: 0.3rem;">ğŸ“„ Professional Documents ({professional_count})</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">CV, work experience, and career-related information for factual responses</p>
            </div>
            <div style="margin-bottom: 1rem;">
                <h4 style="color: #8e44ad; margin-bottom: 0.3rem;">ğŸ§  Mindset Analysis ({mindset_count})</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Communication patterns and thinking style (personal details protected)</p>
            </div>
            <div>
                <h4 style="color: #e67e22; margin-bottom: 0.3rem;">ğŸ“š General Knowledge ({general_count})</h4>
                <p style="color: #7f8c8d; font-size: 0.9rem;">Additional context and supporting information</p>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)

with col2:
    # Enhanced Q&A Section
    st.markdown("### ğŸ§  Ask the Enhanced Reasoning Assistant")
    st.markdown("Ask complex questions that require deep analysis and reasoning:")
    
    # Direct Q&A input box
    user_question = st.text_input(
        "Your Complex Question:",
        placeholder="Ask analytical questions like 'Why is Rory successful?' or 'How does his approach differ?'",
        key="reasoning_question_input"
    )
    
    if st.button("ğŸ§  Analyze & Reason", key="ask_reasoning_question") and user_question.strip():
        st.session_state.selected_question = user_question.strip()
        st.rerun()
    
    st.markdown("---")
    
    # Enhanced Sample Questions
    st.markdown("### ğŸ’¡ Complex Reasoning Questions - Click to Explore!")
    
    reasoning_questions = [
        "åˆ†æRoryçš„è·æ¥­ç™¼å±•æ¨¡å¼å’ŒæˆåŠŸå› ç´ æ˜¯ä»€éº¼ï¼Ÿ",
        "æ¯”è¼ƒRoryåœ¨ä¸åŒè¡Œæ¥­çš„ç¶“é©—ï¼Œä»–æœ‰ä»€éº¼ç¨ç‰¹å„ªå‹¢ï¼Ÿ",
        "åŸºæ–¼Roryçš„èƒŒæ™¯ï¼Œé æ¸¬ä»–åœ¨AIé ˜åŸŸçš„ç™¼å±•æ–¹å‘",
        "Roryè§£æ±ºè¤‡é›œæ•¸æ“šç§‘å­¸å•é¡Œçš„æ–¹æ³•è«–æ˜¯ä»€éº¼ï¼Ÿ",
        "ç‚ºä»€éº¼Roryèƒ½å¤ åœ¨æŠ€è¡“å’Œç®¡ç†ä¹‹é–“å–å¾—å¹³è¡¡ï¼Ÿ",
        "åˆ†æRoryçš„è·¨è¡Œæ¥­ç¶“é©—å¦‚ä½•å½±éŸ¿ä»–çš„æ€ç¶­æ–¹å¼",
        "Roryçš„å‰µæ–°èƒ½åŠ›é«”ç¾åœ¨å“ªäº›æ–¹é¢ï¼Ÿè«‹æ·±åº¦åˆ†æ",
        "å¦‚ä½•è©•ä¼°Roryåœ¨æ•¸æ“šç§‘å­¸é ˜åŸŸçš„ç«¶çˆ­å„ªå‹¢ï¼Ÿ",
        "é æ¸¬Roryæœªä¾†5å¹´çš„è·æ¥­ç™¼å±•è»Œè·¡å’Œå¯èƒ½æ€§"
    ]
    
    # Create clickable reasoning questions
    for i, question in enumerate(reasoning_questions):
        if st.button(f"ğŸ§  {question}", key=f"reasoning_q_{i}", help="Click for deep analysis"):
            st.session_state.selected_question = question
            st.rerun()
    
    # Enhanced Chat Interface
    st.markdown("---")
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¬ Reasoning Chat Interface")
    
    # Initialize chat history
    if "reasoning_messages" not in st.session_state:
        st.session_state.reasoning_messages = []
        # Add enhanced welcome message
        st.session_state.reasoning_messages.append({
            "role": "assistant", 
            "content": """ğŸ§  **æ­¡è¿ä½¿ç”¨Roryçš„å¢å¼·æ¨ç†åŠ©æ‰‹ï¼**

æˆ‘å…·å‚™é«˜ç´šåˆ†æèƒ½åŠ›ï¼Œå¯ä»¥ï¼š
â€¢ **æ·±åº¦æ¨ç†**: åˆ†æèƒŒå¾Œçš„é‚è¼¯å’Œæ¨¡å¼
â€¢ **æ¯”è¼ƒåˆ†æ**: è­˜åˆ¥å„ªå‹¢å’Œå·®ç•°åŒ–å› ç´   
â€¢ **é æ¸¬æ¨ç†**: åŸºæ–¼ç¾æœ‰ä¿¡æ¯é€²è¡Œåˆç†æ¨æ¸¬
â€¢ **æˆ°ç•¥æ€ç¶­**: æä¾›æ–¹æ³•è«–å’Œè§£æ±ºæ–¹æ¡ˆ

æˆ‘æœƒåŸºæ–¼Roryçš„å°ˆæ¥­å±¥æ­·é€²è¡Œåˆ†æï¼ŒåŒæ™‚å­¸ç¿’ä»–çš„æ€ç¶­æ¨¡å¼ä¾†æä¾›æ›´æœ‰æ´å¯ŸåŠ›çš„å›ç­”ã€‚è«‹å•ä¸€äº›éœ€è¦æ·±åº¦åˆ†æçš„è¤‡é›œå•é¡Œï¼"""
        })

    # Handle selected question from sample questions
    if "selected_question" in st.session_state:
        selected_q = st.session_state.selected_question
        st.session_state.reasoning_messages.append({"role": "user", "content": selected_q})
        
        # Show enhanced processing indicator
        with st.spinner("ğŸ§  é€²è¡Œæ·±åº¦æ¨ç†åˆ†æä¸­..."):
            try:
                result = reasoning_chain({"query": selected_q})
                
                # Extract the result properly
                if isinstance(result, dict):
                    response = result.get("result", "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°ç›¸é—œçš„åˆ†æçµæœã€‚")
                else:
                    response = str(result)
                
                # Add AI response to chat history
                st.session_state.reasoning_messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                error_msg = f"æŠ±æ­‰ï¼Œåœ¨è™•ç†æ‚¨çš„å•é¡Œæ™‚é‡åˆ°äº†éŒ¯èª¤ï¼š{str(e)}"
                st.session_state.reasoning_messages.append({"role": "assistant", "content": error_msg})
        
        del st.session_state.selected_question
        st.rerun()

    # Display chat messages
    for message in st.session_state.reasoning_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Enhanced chat input
    if prompt := st.chat_input("å•ä¸€å€‹éœ€è¦æ·±åº¦åˆ†æçš„è¤‡é›œå•é¡Œ..."):
        # Add user message to chat history
        st.session_state.reasoning_messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate enhanced reasoning response
        with st.chat_message("assistant"):
            # Show enhanced processing indicator
            with st.spinner("ğŸ§  åŸ·è¡Œé«˜ç´šæ¨ç†åˆ†æ..."):
                try:
                    result = reasoning_chain({"query": prompt})
                    
                    # Extract the result properly
                    if isinstance(result, dict):
                        response = result.get("result", "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°ç›¸é—œçš„åˆ†æçµæœã€‚")
                    else:
                        response = str(result)
                    
                    # Display response
                    st.markdown(response)
                    
                    # Add assistant response to chat history
                    st.session_state.reasoning_messages.append({"role": "assistant", "content": response})
                    
                except Exception as e:
                    error_msg = f"æŠ±æ­‰ï¼Œåœ¨è™•ç†æ‚¨çš„å•é¡Œæ™‚é‡åˆ°äº†éŒ¯èª¤ï¼š{str(e)}"
                    st.error(error_msg)
                    st.session_state.reasoning_messages.append({"role": "assistant", "content": error_msg})

    st.markdown('</div>', unsafe_allow_html=True)

# Enhanced Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; padding: 2rem; color: #666;">
    <p>ğŸ§  <strong>Rory's Enhanced Reasoning Assistant</strong> | Powered by Advanced AI & Deep Learning</p>
    <p>ğŸ” <strong>Features:</strong> Document Type Separation | Advanced Reasoning | Pattern Analysis | Predictive Insights</p>
    <p>ğŸ“§ For professional inquiries, please ask the AI for Rory's contact information</p>
    <p><em>This AI assistant uses sophisticated reasoning algorithms to provide deep analytical insights based on professional documentation</em></p>
</div>
""", unsafe_allow_html=True)
